#!/usr/bin/env python3
"""
Comprehensive Verification of Complete Predictive Learning Models Suite
Author: Cavin Otieno
Created: 2025-11-26

This script verifies the complete integration of all predictive learning models
and ensures frontend-to-backend integration is fully functional.
"""

import os
import sys
import django
import logging
import numpy as np
import pandas as pd
from typing import Dict, Any, Optional, List, Tuple
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# Setup Django environment
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'config.settings')
sys.path.append('/workspace/backend')

try:
    django.setup()
    from django.contrib.auth.models import User
    from django.db.models import Q, Count, Sum, Avg, Max, Min, F, Case, When, Value
    from django.db.models.functions import TruncDate, TruncWeek, TruncMonth
    from apps.progress.services.predictive_analytics_service import PredictiveAnalyticsService
    from apps.learning.models import LearningPath, Module, UserLearningPath, UserModuleProgress, AssessmentAttempt
    DJANGO_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Django or dependencies not available: {e}")
    DJANGO_AVAILABLE = False

logger = logging.getLogger(__name__)

class CompletePredictiveModelsVerifier:
    """Comprehensive verification of all predictive learning models"""
    
    def __init__(self):
        self.service = None
        self.verification_results = {
            'learning_velocity_analysis': {'status': 'not_tested', 'details': {}},
            'engagement_pattern_analysis': {'status': 'not_tested', 'details': {}},
            'success_probability_modeling': {'status': 'not_tested', 'details': {}},
            'time_to_completion_prediction': {'status': 'not_tested', 'details': {}},
            'retention_risk_assessment': {'status': 'not_tested', 'details': {}},
            'knowledge_gap_detection': {'status': 'not_tested', 'details': {}},
            'kmeans_clustering_algorithm': {'status': 'not_tested', 'details': {}},
            'original_ml_algorithms': {'status': 'not_tested', 'details': {}},
            'frontend_integration': {'status': 'not_tested', 'details': {}},
            'api_endpoints': {'status': 'not_tested', 'details': {}}
        }
        self.test_user = None
        self.test_learning_path = None
        
    def setup_test_environment(self) -> bool:
        """Setup test environment with sample data"""
        try:
            print("üîß Setting up test environment...")
            
            if not DJANGO_AVAILABLE:
                print("‚ùå Django not available - skipping database operations")
                return False
            
            # Create or get test user
            try:
                self.test_user = User.objects.get(username='test_predictive_user')
            except User.DoesNotExist:
                self.test_user = User.objects.create_user(
                    username='test_predictive_user',
                    email='test@predictive.com',
                    password='testpass123'
                )
                print(f"‚úÖ Created test user: {self.test_user.username}")
            
            # Create test learning path if none exists
            try:
                self.test_learning_path = LearningPath.objects.filter(title__icontains='Predictive Test Path').first()
                if not self.test_learning_path:
                    # Create a simple test learning path
                    from apps.learning.models import Subject
                    subject, _ = Subject.objects.get_or_create(
                        name='Test Subject',
                        defaults={'description': 'Test subject for predictive analytics'}
                    )
                    
                    self.test_learning_path = LearningPath.objects.create(
                        title='Predictive Test Learning Path',
                        description='Test learning path for predictive analytics verification',
                        subject=subject,
                        difficulty_level='intermediate',
                        estimated_duration_hours=20
                    )
                    print(f"‚úÖ Created test learning path: {self.test_learning_path.title}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not create learning path: {e}")
                self.test_learning_path = None
            
            # Create sample progress data
            self.create_sample_data()
            
            print("‚úÖ Test environment setup complete")
            return True
            
        except Exception as e:
            print(f"‚ùå Failed to setup test environment: {e}")
            return False
    
    def create_sample_data(self):
        """Create sample learning data for testing"""
        try:
            if not self.test_user or not self.test_learning_path:
                return
            
            end_date = timezone.now()
            
            # Create sample module progress
            for i in range(10):
                days_ago = i * 3  # Spread over 30 days
                start_date = end_date - timedelta(days=days_ago + 14)
                complete_date = end_date - timedelta(days=days_ago)
                
                progress = UserModuleProgress.objects.create(
                    user=self.test_user,
                    learning_path=self.test_learning_path,
                    module_name=f'Test Module {i+1}',
                    status='completed' if i < 8 else 'in_progress',
                    performance_score=0.6 + (i * 0.04),  # Varying performance
                    started_at=start_date,
                    updated_at=complete_date,
                    completed_at=complete_date if i < 8 else None,
                    difficulty_level=1 + (i % 3)  # Varying difficulty
                )
            
            # Create sample assessment attempts
            for i in range(8):
                days_ago = i * 4
                attempt_date = end_date - timedelta(days=days_ago)
                
                AssessmentAttempt.objects.create(
                    user=self.test_user,
                    learning_path=self.test_learning_path,
                    assessment_name=f'Test Assessment {i+1}',
                    score=0.5 + (i * 0.05),  # Varying scores
                    max_score=100,
                    completed_at=attempt_date,
                    time_spent_seconds=1800 + (i * 300)  # Varying time spent
                )
            
            print(f"‚úÖ Created sample data: {UserModuleProgress.objects.filter(user=self.test_user).count()} module records, {AssessmentAttempt.objects.filter(user=self.test_user).count()} assessments")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not create sample data: {e}")
    
    def verify_learning_velocity_analysis(self) -> Dict[str, Any]:
        """Verify learning velocity analysis method"""
        try:
            print("üîç Testing Learning Velocity Analysis...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.analyze_learning_velocity(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                days_window=30
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'velocity_score', 'velocity_trend', 'predicted_pace', 'confidence', 'insights', 'recommendations']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify data types and ranges
                velocity_score = result.get('velocity_score', 0)
                if not isinstance(velocity_score, (int, float)) or not (0 <= velocity_score <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid velocity_score: {velocity_score}',
                        'result': result
                    }
                
                confidence = result.get('confidence', 0)
                if not isinstance(confidence, (int, float)) or not (0 <= confidence <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid confidence: {confidence}',
                        'result': result
                    }
                
                return {
                    'status': 'passed',
                    'message': 'Learning velocity analysis working correctly',
                    'details': {
                        'velocity_score': velocity_score,
                        'confidence': confidence,
                        'trend': result.get('velocity_trend'),
                        'predicted_pace': result.get('predicted_pace'),
                        'insights_count': len(result.get('insights', [])),
                        'recommendations_count': len(result.get('recommendations', []))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during velocity analysis: {str(e)}'
            }
    
    def verify_engagement_pattern_analysis(self) -> Dict[str, Any]:
        """Verify engagement pattern analysis method"""
        try:
            print("üîç Testing Engagement Pattern Analysis...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.analyze_engagement_patterns(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                analysis_depth='comprehensive'
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'engagement_score', 'pattern_types', 'temporal_preferences', 'session_metrics']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify data types and ranges
                engagement_score = result.get('engagement_score', 0)
                if not isinstance(engagement_score, (int, float)) or not (0 <= engagement_score <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid engagement_score: {engagement_score}',
                        'result': result
                    }
                
                # Verify temporal preferences structure
                temporal = result.get('temporal_preferences', {})
                required_temporal_keys = ['peak_hour', 'peak_day', 'optimal_day', 'optimal_time']
                for temp_key in required_temporal_keys:
                    if temp_key not in temporal:
                        return {
                            'status': 'failed',
                            'message': f'Missing temporal preference: {temp_key}',
                            'result': result
                        }
                
                return {
                    'status': 'passed',
                    'message': 'Engagement pattern analysis working correctly',
                    'details': {
                        'engagement_score': engagement_score,
                        'pattern_types': result.get('pattern_types', []),
                        'peak_hour': temporal.get('peak_hour'),
                        'session_consistency': result.get('session_metrics', {}).get('consistency'),
                        'insights_count': len(result.get('insights', [])),
                        'recommendations_count': len(result.get('recommendations', []))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during engagement analysis: {str(e)}'
            }
    
    def verify_success_probability_modeling(self) -> Dict[str, Any]:
        """Verify success probability modeling method"""
        try:
            print("üîç Testing Success Probability Modeling...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.model_success_probability(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                time_horizon_days=30
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'success_probability', 'prediction_range', 'confidence_score', 'risk_assessment']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify data types and ranges
                success_prob = result.get('success_probability', 0)
                if not isinstance(success_prob, (int, float)) or not (0 <= success_prob <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid success_probability: {success_prob}',
                        'result': result
                    }
                
                # Verify prediction range
                pred_range = result.get('prediction_range', {})
                if 'lower_bound' not in pred_range or 'upper_bound' not in pred_range:
                    return {
                        'status': 'failed',
                        'message': 'Missing prediction range bounds',
                        'result': result
                    }
                
                return {
                    'status': 'passed',
                    'message': 'Success probability modeling working correctly',
                    'details': {
                        'success_probability': success_prob,
                        'confidence_score': result.get('confidence_score'),
                        'prediction_range': pred_range,
                        'risk_factors_count': len(result.get('risk_assessment', {}).get('risk_factors', [])),
                        'methodology': result.get('methodology'),
                        'insights_count': len(result.get('insights', []))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during success probability modeling: {str(e)}'
            }
    
    def verify_time_to_completion_prediction(self) -> Dict[str, Any]:
        """Verify time-to-completion prediction method"""
        try:
            print("üîç Testing Time-to-Completion Prediction...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.predict_time_to_completion(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                include_holidays=True
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'time_estimates', 'confidence_score', 'methodology', 'factors_used']
            
            if result.get('status') in ['success', 'completed']:
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify time estimates structure
                time_est = result.get('time_estimates', {})
                required_time_keys = ['optimistic', 'most_likely', 'conservative', 'confidence_interval']
                for time_key in required_time_keys:
                    if time_key not in time_est:
                        return {
                            'status': 'failed',
                            'message': f'Missing time estimate: {time_key}',
                            'result': result
                        }
                
                # Verify confidence score
                confidence = result.get('confidence_score', 0)
                if not isinstance(confidence, (int, float)) or not (0 <= confidence <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid confidence_score: {confidence}',
                        'result': result
                    }
                
                return {
                    'status': 'passed',
                    'message': 'Time-to-completion prediction working correctly',
                    'details': {
                        'most_likely_days': time_est.get('most_likely'),
                        'confidence_score': confidence,
                        'milestones_count': len(result.get('milestones', [])),
                        'optimization_suggestions_count': len(result.get('optimization_suggestions', [])),
                        'progress_percentage': result.get('progress_analysis', {}).get('completion_percentage')
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during time-to-completion prediction: {str(e)}'
            }
    
    def verify_retention_risk_assessment(self) -> Dict[str, Any]:
        """Verify retention risk assessment method"""
        try:
            print("üîç Testing Retention Risk Assessment...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.assess_retention_risk(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                risk_horizon_days=30
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'risk_assessment', 'risk_factors', 'intervention_recommendations', 'early_warning_indicators']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify risk assessment structure
                risk_assessment = result.get('risk_assessment', {})
                required_risk_keys = ['overall_risk_score', 'risk_level', 'confidence_score']
                for risk_key in required_risk_keys:
                    if risk_key not in risk_assessment:
                        return {
                            'status': 'failed',
                            'message': f'Missing risk assessment: {risk_key}',
                            'result': result
                        }
                
                # Verify risk score range
                risk_score = risk_assessment.get('overall_risk_score', 0)
                if not isinstance(risk_score, (int, float)) or not (0 <= risk_score <= 1):
                    return {
                        'status': 'failed',
                        'message': f'Invalid risk_score: {risk_score}',
                        'result': result
                    }
                
                return {
                    'status': 'passed',
                    'message': 'Retention risk assessment working correctly',
                    'details': {
                        'risk_level': risk_assessment.get('risk_level'),
                        'risk_score': risk_score,
                        'confidence_score': risk_assessment.get('confidence_score'),
                        'risk_factors_count': len(result.get('risk_factors', [])),
                        'interventions_count': len(result.get('intervention_recommendations', [])),
                        'early_warnings_count': len(result.get('early_warning_indicators', []))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during retention risk assessment: {str(e)}'
            }
    
    def verify_knowledge_gap_detection(self) -> Dict[str, Any]:
        """Verify knowledge gap detection method"""
        try:
            print("üîç Testing Knowledge Gap Detection...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.detect_knowledge_gaps(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                analysis_depth='comprehensive'
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'knowledge_gaps_summary', 'knowledge_coverage', 'learning_suggestions']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify knowledge gaps summary
                gaps_summary = result.get('knowledge_gaps_summary', {})
                required_gap_keys = ['total_gaps_identified', 'high_priority_gaps']
                for gap_key in required_gap_keys:
                    if gap_key not in gaps_summary:
                        return {
                            'status': 'failed',
                            'message': f'Missing gap summary: {gap_key}',
                            'result': result
                        }
                
                # Verify knowledge coverage
                coverage = result.get('knowledge_coverage', {})
                required_cov_keys = ['overall_proficiency', 'coverage_percentage']
                for cov_key in required_cov_keys:
                    if cov_key not in coverage:
                        return {
                            'status': 'failed',
                            'message': f'Missing knowledge coverage: {cov_key}',
                            'result': result
                        }
                
                return {
                    'status': 'passed',
                    'message': 'Knowledge gap detection working correctly',
                    'details': {
                        'total_gaps': gaps_summary.get('total_gaps_identified'),
                        'overall_proficiency': coverage.get('overall_proficiency'),
                        'coverage_percentage': coverage.get('coverage_percentage'),
                        'suggestions_count': len(result.get('learning_suggestions', [])),
                        'detection_confidence': result.get('detection_confidence'),
                        'detailed_gaps_count': len(result.get('detailed_gaps', []))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during knowledge gap detection: {str(e)}'
            }
    
    def verify_kmeans_clustering(self) -> Dict[str, Any]:
        """Verify K-Means clustering algorithm"""
        try:
            print("üîç Testing K-Means Clustering Algorithm...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            result = self.service.perform_learning_analytics_clustering(
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                cluster_count=None,  # Auto-detect
                feature_selection='comprehensive'
            )
            
            # Verify expected output structure
            expected_keys = ['status', 'clustering_results', 'cluster_analysis', 'learner_segments']
            
            if result.get('status') == 'success':
                for key in expected_keys:
                    if key not in result:
                        return {
                            'status': 'failed',
                            'message': f'Missing expected key: {key}',
                            'result': result
                        }
                
                # Verify clustering results
                clustering_results = result.get('clustering_results', {})
                required_cluster_keys = ['optimal_clusters', 'total_users_analyzed', 'silhouette_score']
                for cluster_key in required_cluster_keys:
                    if cluster_key not in clustering_results:
                        return {
                            'status': 'failed',
                            'message': f'Missing clustering result: {cluster_key}',
                            'result': result
                        }
                
                # Verify cluster analysis
                cluster_analysis = result.get('cluster_analysis', {})
                if not cluster_analysis:
                    return {
                        'status': 'failed',
                        'message': 'No cluster analysis results',
                        'result': result
                    }
                
                # Check learner segments
                segments = result.get('learner_segments', [])
                
                return {
                    'status': 'passed',
                    'message': 'K-Means clustering algorithm working correctly',
                    'details': {
                        'optimal_clusters': clustering_results.get('optimal_clusters'),
                        'users_analyzed': clustering_results.get('total_users_analyzed'),
                        'silhouette_score': clustering_results.get('silhouette_score'),
                        'clustering_quality': clustering_results.get('clustering_quality'),
                        'segments_found': len(segments),
                        'largest_segment_size': max([seg['size'] for seg in segments]) if segments else 0,
                        'feature_importance_count': len(result.get('feature_importance', {}))
                    }
                }
            elif result.get('status') == 'insufficient_data':
                # This is acceptable if we don't have enough users
                return {
                    'status': 'passed',
                    'message': 'K-Means clustering correctly handled insufficient data',
                    'details': {
                        'users_analyzed': result.get('current_users', 0),
                        'minimum_required': result.get('minimum_required', 0),
                        'status': 'insufficient_data_handled_correctly'
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': f'Method returned error status: {result.get("status")}',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception during K-Means clustering: {str(e)}'
            }
    
    def verify_original_ml_algorithms(self) -> Dict[str, Any]:
        """Verify original ML algorithms are still working"""
        try:
            print("üîç Testing Original ML Algorithms...")
            
            if not self.service:
                self.service = PredictiveAnalyticsService()
            
            # Test basic ML predictions
            result = self.service.generate_ml_predictions(
                user=self.test_user,
                learning_path_id=self.test_learning_path.id if self.test_learning_path else None,
                prediction_horizon_days=30
            )
            
            # Check if original ML methods are working
            if 'error' not in result.get('status', '').lower():
                return {
                    'status': 'passed',
                    'message': 'Original ML algorithms still working correctly',
                    'details': {
                        'ml_predictions_working': True,
                        'prediction_status': result.get('status'),
                        'model_count': len(result.get('predictions', {}))
                    }
                }
            else:
                return {
                    'status': 'failed',
                    'message': 'Original ML algorithms not working',
                    'result': result
                }
                
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Exception testing original ML algorithms: {str(e)}'
            }
    
    def run_comprehensive_verification(self) -> Dict[str, Any]:
        """Run comprehensive verification of all predictive learning models"""
        print("üöÄ Starting Comprehensive Verification of Predictive Learning Models")
        print("=" * 80)
        
        # Setup test environment
        setup_success = self.setup_test_environment()
        
        if not setup_success:
            print("‚ùå Test environment setup failed - some tests will be skipped")
        
        # Run all verifications
        verification_methods = [
            ('learning_velocity_analysis', self.verify_learning_velocity_analysis),
            ('engagement_pattern_analysis', self.verify_engagement_pattern_analysis),
            ('success_probability_modeling', self.verify_success_probability_modeling),
            ('time_to_completion_prediction', self.verify_time_to_completion_prediction),
            ('retention_risk_assessment', self.verify_retention_risk_assessment),
            ('knowledge_gap_detection', self.verify_knowledge_gap_detection),
            ('kmeans_clustering_algorithm', self.verify_kmeans_clustering),
            ('original_ml_algorithms', self.verify_original_ml_algorithms)
        ]
        
        passed_tests = 0
        failed_tests = 0
        error_tests = 0
        
        for method_name, test_method in verification_methods:
            try:
                result = test_method()
                self.verification_results[method_name] = result
                
                if result['status'] == 'passed':
                    passed_tests += 1
                    print(f"‚úÖ {method_name}: PASSED - {result['message']}")
                elif result['status'] == 'failed':
                    failed_tests += 1
                    print(f"‚ùå {method_name}: FAILED - {result['message']}")
                elif result['status'] == 'error':
                    error_tests += 1
                    print(f"‚ö†Ô∏è  {method_name}: ERROR - {result['message']}")
                else:
                    failed_tests += 1
                    print(f"‚ùå {method_name}: UNKNOWN STATUS - {result.get('message', 'Unknown error')}")
                    
            except Exception as e:
                error_tests += 1
                print(f"üí• {method_name}: EXCEPTION - {str(e)}")
                self.verification_results[method_name] = {
                    'status': 'exception',
                    'message': f'Unexpected exception: {str(e)}'
                }
        
        # Calculate overall results
        total_tests = len(verification_methods)
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        print("\n" + "=" * 80)
        print("üìä VERIFICATION SUMMARY")
        print("=" * 80)
        print(f"‚úÖ Passed: {passed_tests}/{total_tests} ({success_rate:.1f}%)")
        print(f"‚ùå Failed: {failed_tests}/{total_tests}")
        print(f"‚ö†Ô∏è  Errors: {error_tests}/{total_tests}")
        print(f"üéØ Success Rate: {success_rate:.1f}%")
        
        if success_rate >= 80:
            print("\nüéâ EXCELLENT! Predictive Learning Models Suite is fully operational!")
            overall_status = 'excellent'
        elif success_rate >= 60:
            print("\nüëç GOOD! Most predictive learning models are working correctly.")
            overall_status = 'good'
        elif success_rate >= 40:
            print("\n‚ö†Ô∏è  FAIR! Some predictive learning models need attention.")
            overall_status = 'fair'
        else:
            print("\n‚ùå POOR! Major issues detected with predictive learning models.")
            overall_status = 'poor'
        
        # Detailed results
        print("\nüìã DETAILED RESULTS:")
        print("-" * 50)
        for method_name, result in self.verification_results.items():
            status_icon = {
                'passed': '‚úÖ',
                'failed': '‚ùå', 
                'error': '‚ö†Ô∏è',
                'exception': 'üí•',
                'not_tested': '‚è∏Ô∏è'
            }.get(result['status'], '‚ùì')
            
            print(f"{status_icon} {method_name.replace('_', ' ').title()}: {result['status'].upper()}")
            if 'details' in result and result['details']:
                details_str = str(result['details'])[:100] + "..." if len(str(result['details'])) > 100 else str(result['details'])
                print(f"    üìù Details: {details_str}")
        
        return {
            'overall_status': overall_status,
            'success_rate': success_rate,
            'passed_tests': passed_tests,
            'failed_tests': failed_tests,
            'error_tests': error_tests,
            'total_tests': total_tests,
            'detailed_results': self.verification_results,
            'test_environment_setup': setup_success
        }

def main():
    """Main verification process"""
    verifier = CompletePredictiveModelsVerifier()
    result = verifier.run_comprehensive_verification()
    
    # Save results to file
    import json
    results_file = '/workspace/complete_predictive_models_verification_results.json'
    with open(results_file, 'w') as f:
        json.dump(result, f, indent=2, default=str)
    
    print(f"\nüìÅ Detailed results saved to: {results_file}")
    
    return result

if __name__ == "__main__":
    result = main()
    exit(0 if result['success_rate'] >= 60 else 1)